%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with Harvard-style bibliographic references

\documentclass[preprint,number]{elsarticle}
%\usepackage[utf8]{inputenc}
\usepackage{mathpazo} %Tipo de letra
\usepackage{parskip} %saltos de parrafo
\setlength{\parskip}{\baselineskip}

%% Use the option review to obtain double line spacing
%\documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
 %\documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{amsmath}
%\usepackage{lipsum}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

%% You might want to define your own abbreviated commands for common used terms, e.g.:
%\newcommand{\kms}{km\,s$^{-1}$}
%\newcommand{\msun}{$M_\odot$}

\journal{Electronic Journal}


\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%            addressline={}, 
%%            city={},
%%            postcode={}, 
%%            state={},
%%            country={}}
%% \fntext[label3]{}

\title{Comparativa de Redes Neuronales Profundas (RNN, CNN y GNN) para la Clasificación de Tipos de Cáncer usando Datos Multiómicos Integrados.}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[label1]{Figueroa Arcos Arturo}
\affiliation[label1]{organization={Centro Nacional de Investigación y Desarrollo Tecnológico},%Department and Organization
            addressline={interior internado palmira}, 
            city={Cuernavaca},
            postcode={62493}, 
            state={Morelos},
            country={México}}

\author[label2]{Alvarado Martinez Victor M.}
\affiliation[label2]{organization={Centro Nacional de Investigación y Desarrollo Tecnológico},%Department and Organization
            addressline={interior internado palmira}, 
            city={Cuernavaca},
            postcode={62493}, 
            state={Morelos},
            country={México}}

\begin{abstract}
%% Text of abstract
Problema: Destacar la necesidad de metodos precisos para clasificar tipos de cancer y aprovechar la informacion de los datos omicos.

Metodo: Evaluacion comparativa de los modelos CNN, RNN y GNN con la integracion de datos omicos (mRNA, miRNA y metilacion de ADN).

Resultados: Mostrar el rendimiento de los modelos mediante las metricas.

Conclusion: El hallazgo del potencial de las redes neuronales GNN para capturar relaciones biológicas de los datos multiómicos.

\end{abstract}

%%Graphical abstract
%\begin{graphicalabstract}
%\includegraphics{grabs}
%\end{graphicalabstract}

%%Research highlights
%\begin{highlights}
%\item Research highlight 1
%\item Research highlight 2
%\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword, up to a maximum of 6 keywords
keyword 1 \sep keyword 2 \sep keyword 3 \sep keyword 4

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}


\end{frontmatter}

%\tableofcontents

%% \linenumbers

%% main text

\section{Introducción}
\label{Introducción}

El cáncer es una de las principales causas de muerte a nivel mundial, catalogándose como una de las crisis más importantes de salud pública y persistente de la era moderna \cite{OMS_Cancer_2023_online}. Según las estadísticas globales más recientes, en 2020 se registraron casi 10 millones de muertes atribuidas al cáncer, y las estimaciones para el 2022 indican una cifra similar, de cerca 9.7 millones de muertes \cite{NCI_CancerStats_2025_online}. A nivel mundial, el cáncer de pulmón, mama y colorrectal son los más prevalentes, que constituyen colectivamente alrededor de dos tercios de los nuevos casos y muertes reportadas en 2022 \cite{siegel2023cancer}. Cabe destacar que se ha reportado también un descenso notable en la tasa de mortalidad, con una reducción del 33\% debido a los avances en tratamiento y detección temprana, al igual que varios tipos de cáncer clave están en aumento. Específicamente, se ha observado un incremento anual del 3\% en la incidencia de cáncer de próstata entre 2014 y 2019, junto con aumentos continuos en cáncer de mama y de cuerpo uterino \cite{dizon2024cancer}. Debido con estos hechos se obtiene una divergencia entre la mejora de la supervivencia y el aumento de la incidencia, con una situación critica que mientras los avances terapéuticos mejoran la capacidad de tratar la enfermedad establecida, la carga de nuevos diagnósticos no disminuye de manera uniforme. Con este panorama se sugiere que la próxima frontera es el desarrollo de herramientas diagnósticas y de clasificación más precisas y tempranas, que sean capaces de abordar el cáncer en sus etapas moleculares incipientes.

El cáncer es un complejo y heterogéneo conjunto de patologías, debido a un crecimiento celular descontrolado, impulsado por una acumulación de alteraciones genéticas y epigenéticas \cite{hanahan2011hallmarks}. La clasificación de tipos de cáncer es fundamental en el contexto de la medicina de precisión, esto permite la selección de terapias dirigidas, estratificación de pacientes y la mejora de resultados de pronóstico \cite{zhuang2021multi}. Sin embargo, se tiene un desafío debido a la alta similitud hispatológica entre ciertos tumores y la heterogeneidad intratumoral.

Históricamente, la investigación del cáncer se ha centrado en capas individuales de datos biológicos, con mayor frecuencia los datos de expresión génica (transcriptómicos). Sin embargo, una sola capa no es capaz de capturar la complejidad del cáncer, presentando una versión incompleta y en ocasiones, engañosa de la biología tumoral. Este enfoque a menudo solo suele revelar un tipo de información omica a la vez \cite{acharya2024comprehensive}.  De la misma manera, el análisis de una sola omica suele presentar sesgos inherentes a la tecnología y los hallazgos al estudiar una sola capa reduce la  de la validación \cite{massimino2023single}.

Para contrarrestar esta limitante y capturar una visión holística de la biología del cáncer, se ha precisado la integración de múltiples conjuntos de datos ómicos generados a partir de los mismos pacientes. El enfoque de integración multiómica permite descubrir patrones biológicos más complejos y obtener una comprensión funcional completa \cite{hernandez2024methods}. La combinación de datos omicos permite la identificación de subtipos moleculares distintos que podrían permanecer sin ser detectados con el análisis de una sola ómica, proporcionando una caracterización más profunda y precisa de la enfermedad de un paciente \cite{kutlay2021integrative}.

La integración de datos multiómicos a pesar de su potencial, presenta desafíos computacionales. Estos datos presentan el problema de la alta dimensionalidad (el ¨ problema de $p \gg n$¨, donde el número de características $p$, supera con creces el número de muestras $n$) lo que aumenta el riesgo de sobreajuste (overfitting). Además, los datos ómicos son inherentemente ruidosos y a menudo, incompletos. Sin un preprocesamiento riguroso, los modelos de aprendizaje profundo pueden fallar debido al vasto espacio de características y ruido debido a su heterogeneidad y la presencia de complejas interacciones no lineales entre las diferentes capas biológicas \cite{libbrecht2015machine}. Si bien los métodos clásicos de aprendizaje automático (como SVM o Random Forest) han demostrado cierto éxito, a menudo luchan por modelar eficazmente estas no linealidades y la jerarquía de las interacciones biológicas.

En este contexto. el Aprendizaje Profundo (DL) surge como una poderosa herramienta del subcampo de las redes Neuronales Artificiales (ANN), capaz de aprender de representaciones jerárquicas y patrones complejos a partir de datos de alta dimensionalidad \cite{lecun2015deep}. La literatura reciente ha demostrado la aplicación exitosa de modelos de DL en una variedad de tareas oncológicas, como la clasificación de tipos y subtipos de cáncer, identificación de genes conductores y predicción de la supervivencia utilizando datos omicos de fuentes como TCGA \cite{sartori2025comprehensive}. 

El campo del DL no está definido por un solo modelo, si no por una comparación de arquitecturas para identificar cuál es la que se adapta mejor a la estructura de datos biológicos \cite{sartori2025comprehensive}. Se han aplicado diversas arquitecturas a los datos omicos como por ejemplo las Redes Neuronales Convolucionales (CNN), diseñadas originalmente para el procesamiento de imágenes, se han adaptado para encontrar patrones locales en los datos genómicos, a menudo tratando el vector de características ómicas como una imagen de 1D o una matriz de características de 2D \cite{ye2021genomic,chuang2021convolutional}. Por su parte, las Redes Neuronales Recurrentes (RNN), como las LSTM o GRU, están diseñadas para datos secuenciales y pueden, en teoría, capturar dependencias a largo plazo dentro del vector de características concatenadas, aunque esta representación secuencial es una abstracción de la biología subyacente \cite{parthasarathy2025novel,barbadilla2025predicting}.

Sin embargo, en las CNN como en las RNN se tienen la limitación que se tratan las características ómicas (genes, miARNs, sitios CpG) como una secuencia o cuadricula arbitraria. Es decir, que ignoran en gran medida la topología biológica inherente, que los genes y sus productos no actúan de forma aislada, sino dentro de redes de interacción complejas (rutas metabólicas, redes de regulación génica, interacciones proteína-proteína).

Las Redes Neuronales de Grafos (GNN) son una opción que ofrece una ventaja conceptual decisiva. Las GNN son una arquitectura que aprendizaje profundo que están diseñadas para procesar datos que están estructurados como grafos \cite{jiang2019semi,velickovic2017graph}. A diferencia de las CNN o RNN, el sesgo inductivo de una GNN es la conectividad relacional. Su poder reside en su capacidad para modelar dependencias complejas y capturar relaciones de orden superior de las redes biológicas mediante la agregacion iterativa de información de los nodos vecindarios de los nodos. Al tener las características ómicas y representarlas como nodos en un grafo y sus interacciones conocidas (o inferidas) como ejes, las GNN son capaces de aprender de las representaciones de los nodos agregando información de sus vecindarios biológicamente relevantes. 

%Si bien existen trabajos que aplican modelos CNN, RNN o GNN de forma aislada para clasificación de tipos cáncer, se cuenta con limitada información en cuanto una comparación sistemática y directa de estas tres arquitecturas bajo un mismo conjunto de datos multiómicos y una misma tarea de clasificación multiclase (31 tipos de cáncer y muestras normales).
El objetivo de este trabajo es hacer una comparación del rendimiento de las arquitecturas CNN, RNN y GNN en una tarea de clasificación de 31 tipos de cáncer y muestras normales, utilizando un único conjunto de datos multiómicos integrados, que fueron preprocesados con un análisis de DGE/CpG para selección de genes relevantes, y selección y reducción de características mediante LASSO, la estrategia de representación de los datos fue de matriz de 2D para los modelos CNN, vector de 1D para el modelo RNN y grafo de correlación para los modelos GNN.


\section{Metodología}
%%\label{}

Los datos multiómicos de los 31 tipos de cáncer utilizados en este trabajo se tomaron de proyecto Pan-cancer del Atlas del Genoma de Cáncer (TCGA) \cite{weinstein2013cancer}. Utilizando la biblioteca TCGAbiolinks en R que realiza consultas con la herramienta GDC (Genomic Data Commons), el cual es parte de un proyecto dedicado  a proporcionar una base de datos centralizada para proyectos de investigación en estudios genómicos del cáncer \cite{colaprico2016tcgabiolinks}. Los tipos de cáncer de los tumores y las muestras normales por cada muestra de miARN, ARNm y metilación de ADN se muestran en la siguiente Figura \ref{fig:muestras_TCGA}. El conjunto inicial total consta de 10,511 
muestras de ARNm, 10,048 muestras de miARN y 8,696 muestras de metilación de ADN.


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Imagenes/Muestras_TCGA.png}
    \caption{Numero datos multiómicos total (ARNm, miARN y metilación de ADN) y su distribución por cada tipo muestra tumoral y normal, incluidos en el proyecto Pan-cáncer de los 31 tipos de cancer}
    \label{fig:muestras_TCGA}
\end{figure}

Los datos en bruto obtenidos cuentan con características (genes) para cada dato ómico, para los datos de ARNm se tienen 60,660 características, para los datos de miARN se tienen 1,881 características y para los datos de metilación de ADN se tienen 485,577 características como se muestra en la Figura \ref{fig:muestrasxgenes}.


\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Imagenes/muestrasxgenes.png}
    \caption{Número datos multiómicos total (ARNm, miARN y metilación de ADN) y su distribución por cada tipo muestra tumoral y normal, incluidos en el proyecto Pan-cáncer }
    \label{fig:muestrasxgenes}
\end{figure}

\subsection{Selección de características y reducción dimensional}

Los datos en bruto inicial, abarca decenas de miles de genes y sitios CpG, que contienen una gran cantidad de variables no informáticas o ruidosas. Para mitigar la alta dimensionalidad y mejorar la convergencia de los modelos de DL, se aplicó una estrategia de filtrado jerárquico.

El preprocesamiento de datos en bruto se realizó la identificación de características biológicas relevantes y la reducción de dimensionalidad, para extraer características biológicas importantes de datos de alta dimensionalidad \cite{alharbi2025comparative}. Para la identificación biomarcadores que muestran diferencias significativas en los datos de ARNm y metilación se utilizó el análisis de expresión diferencial (DGE) y análisis de metilación diferencial con la herramienta LIMMA. El análisis DGE se utilizó con el paquete de R DESeq2, que consiste en modelar los datos de ARNm usando distribución binomial negativa, para identificar genes que presentan cambios significativos en los niveles de expresión génica \cite{robinson2010edger}. LIMMA se utilizó con el paquete en R, que asume que los datos siguen una distribución normal (Gaussiana) para identificar sitios CpG metilados diferencialmente significativos \cite{ritchie2015limma}. Posteriormente, LASSO fue aplicado en las características resultantes del análisis diferencial para extender la selección de características de los datos de ARNm y metilación de ADN y reducir las  \cite{tibshirani1996regression}, en la Figura \ref{fig:Proc_datos} se muestra en flujo del procesamiento de los datos.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Imagenes/Proc_datos.png}
    \caption{Número datos multiómicos total (ARNm, miARN y metilación de ADN) y su distribución por cada tipo muestra tumoral y normal, incluidos en el proyecto Pan-cáncer }
    \label{fig:Proc_datos}
\end{figure}

\subsubsection{Análisis de expresión génica diferencial (DGE)}

El Análisis de Expresión Diferencial (DGE) es una metodología fundamental en genómica para comparar los niveles de expresión génica bajo diferentes condiciones biológicas (ej. tratamiento vs. control, o tejido normal vs. tejido tumoral). Esta herramienta permite identificar y caracterizar los genes que modifican su actividad entre la condición de referencia y la experimental.

Dado que los datos de secuenciación (ARNm) presentan una naturaleza de conteo discreto y exhiben sobredispersión, se utilizó el paquete DESeq2. Este método proporciona un marco estadístico robusto basado en Modelos Lineales Generalizados (GLM) que asumen una distribución binomial negativa para los conteos de lecturas. La evaluación de la significancia estadística de los cambios en la expresión entre muestras de tejido normal y muestras de tejido tumoral se realizó mediante la prueba de Wald. Para el criterio de selección, se definieron como diferencialmente expresados únicamente aquellos genes que cumplieron con una significancia estadística estricta, definida por un umbral de valor p ajustado (FDR) < 0.001.

\subsubsection{Análisis de metilación diferencial}

Para la identificación de alteraciones epigenéticas significativas, se utilizó la herramienta LIMMA (Linear Models for Microarray Data), mediante la cual se ajustó un modelo lineal a los niveles de metilación de cada sitio CpG en función de los grupos de muestras experimentales (muestras de tejido tumorales vs muestras de tejido normal). Este enfoque estadístico se basa en estimaciones de varianza mediante el método Bayesiano empírico, con el cual se evalúa la asociación entre el estado de metilación y el fenotipo clínico de manera robusta, estabilizando los errores estándar a través de los miles de sitios.

La selección de los sitios CpG diferencialmente metilados se realizó aplicando criterios estrictos de significancia estadística y magnitud del efecto. Se consideraron candidatos relevantes aquellos sitios que presentaron un valor de p nominal < 0.05 tras el ajuste del modelo. De estos candidatos, se conservan para la etapa de integración multiómica únicamente aquellos que mostraron una diferencia biológica sustancial en los valores Beta entre grupos, asegurando así que las características seleccionadas representaron cambios epigenéticos con potencial impacto funcional en la tumorigénesis.


\subsubsection{LASSO}

Después del filtrado inicial mediante análisis diferencial, se procedió a una reducción de dimensionalidad utilizando la regresión LASSO (Least Absolute Shrinkage and Selection Operator). El método se aplicó de manera independiente a las matrices de datos ARNm y metilación de ADN, permitiendo una selección de características optimizada para la escala y distribución específica de cada perfil ómico antes de su integración. Para cada conjunto de datos, se ajustó el hiperparámetro de regularización ($\lambda$) mediante validación cruzada, seleccionando el valor óptimo que minimizó el error de predicción para identificar el subconjunto más robusto de biomarcadores.

Matemáticamente, el algoritmo selecciona las características minimizando la suma de los residuos cuadrados sujeta a una penalización en la norma $\ell_1$ de los coeficientes, definida por la siguiente función objetivo como:

\begin{equation}
    \min_{\beta} \left\{ \sum_{i=1}^{M} \left(y_i - \sum_{j=1}^{p} x_{ij}\beta_j\right)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
\end{equation}

donde $M$ representa el número de muestras, $p$ el número de características de entrada, y $\lambda$ controla la intensidad de la penalización. Al forzar que los coeficientes de las variables menos relevantes converjan a cero, la aplicación de LASSO actúa como un filtro, únicamente las características con coeficientes $\beta_j \neq 0$ se conservaron.

\subsubsection{Integración y construcción de matriz multiómica}

Tras la selección de características independiente, se procedió a la fusión de los datos de ARNm, miARN y metilación en una estructura de datos unificada. La integración se realizó mediante una operación de unión interna (inner join) basada en los identificadores únicos del paciente, asegurando la correspondencia exacta de las tres modalidades ómicas para cada individuo y excluyendo aquellas muestras que no presentaran información completa.

El conjunto de datos final consolidado (matriz multiómica) quedó constituido por un total de 8,464 muestras y 2,794 características biológicas tras el filtrado. Este registro unifica la diversidad fenotípica de las 32 clases objetivo (31 tipos de cáncer y tejido normal), el cual servirá con entrada estandarizada para el entrenamiento y evaluación comparativa de las arquitecturas de aprendizaje profundo.

\subsection{Estructuración de datos para modelos de aprendizaje profundo}

A partir de la matriz multiómica, constituida por 8,464 muestras y un vector de características de dimensión $N$ = 2,794, se generaron tres representaciones de datos distintas. Esta etapa fue fundamental para adaptar la entrada a los requerimientos estructurales de cada arquitectura de aprendizaje profundo evaluada (espacial, secuencial y relacional).

\subsubsection{Estructuración espacial 2D (CNN)}

Para la implementación de las Redes Neuronales Convolucionales (CNN), el vector de características unidimensional de cada muestra se transformó en una estructura matricial bidimensional, simulando el formato de una imagen. Dado que la dimensión original no corresponde a un cuadrado perfecto, se calculó la dimensión entera más próxima capaz de contener la totalidad de los datos (matriz de $53 \times 53$). Las posiciones vacías restantes en la cuadrícula se completaron mediante técnica de relleno con ceros (zero-padding). En esta disposición, cada pixel representa el nivel de expresión o metilación de una característica específica; sin embargo, es importante notar que la vecindad espacial resultante a causa del reordenamiento no refleja necesariamente proximidad biológica.


\subsubsection{Estructuración secuencial (RNN)}

En la evaluación de los modelos recurrentes mediante la integración de datos multiómicos, se utilizaron dos estrategias para la secuenciación de los datos:

\textit{A. Secuenciación de características globales}

Los datos se tomaron de la matriz y reformateados en un vector concatenado de $N =$ 2,794 características como una secuencia temporal $S_{global} = \{f_1, f_2, ..., f_N\}$. El vector se introdujo al modelo paso a paso. Esta representación asume una dependencia ordinal entre las características individuales. Este enfoque evalúa la capacidad bruta de la RNN para memorizar dependencias de largo alcance en series ruidosas.

\textit{B. Estructuración Jerárquica por Modalidad}

Los datos se organizaron en bloques lógicos, con el fin de facilitar la captura de las interacciones biológicas a diferencia de usar una lista plana y larga de números. El proceso fue el siguiente:

Compresión por tipo de ómica: Los datos se separaron en sus grupos originales: ARNm ($x_{mrna}$), miARN ($x_{mirna}$) y metilación ($x_{meth}$). Cada grupo se procesó de forma independiente a través de capas densas para extraer sus características más relevantes, esta información se guardó en un resumen compacto (embedding) de 512 valores (dimensión $d=512$), generando tres vectores ($e_{mrna}, e_{mirna}, e_{meth}$).

Creación de secuencia: Los vectores resultantes se organizaron para formar una secuencia de solo 3 pasos de tiempo ($T=3$). El orden se definió siguiendo el flujo de la biología molecular, empezando por los datos de metilación, seguidos del miARN y por último el ARNm \cite{jeon2025investigating,fabbri2010epigenetics}. Los embeddings se apilaron para formar la secuencia temporal $S_{latent} = [e_{meth}, e_{mirna}, e_{mrna}]$.

La secuencia resultante se utilizó como entrada al modelo RNN. Con lo que se espera que aprenda la influencia de cada nivel ómico, en lugar de intentar encontrar patrones en una secuencia larga de variables individuales.

\subsubsection{Estructuración basada en grafos (GNN)}

La diferencia con los enfoques anteriores, para las GNN se construyó una estructura topológica basada en relaciones biológicas inferidas. Se definió un grafo $G = (V, E)$, donde el conjunto de los nodos $V$ corresponde de las 2,794 características ómicas. Para definir la conectividad o aristas ($E$), se evaluó la dependencia lineal por pares entre todas las características utilizando el conjunto multiómico. La métrica utilizada fue el coeficiente de correlación de Pearson ($r$), el cual se calculó para dos vectores de características $S_i$ y $S_j$ de acuerdo con la ecuación siguiente:

\begin{equation}
    \text{corr}(S_i, S_j) = \frac{\text{cov}(S_i, S_j)}{\sigma_{S_i} \sigma_{S_j}}
\end{equation}

Donde $\text{cov}$ representa la covarianza y $\sigma$ la desviación estándar de las características respectivas. La matriz de adyacencia $A$ resultante se construyó binarizando estas relaciones; se estableció una conexión física entre nodos $i$ y $j$ únicamente si presentaban una correlación fuerte, aplicando un umbral de corte de $r \le -0.8$ o $r \ge 0.8$. De esta manera, la arquitectura GNN recibe dos entradas: La matriz global de características (atributos del los nodos) y la matriz de conexiones o aristas (matriz de adyacencia). Esta configuración permite que la arquitectura realice operaciones de paso de mensajes (message passing), agregando información de características funcionalmente correlacionadas independientemente de su posición secuencial en el vector de datos original.



\subsection{Arquitecturas de los modelos de aprendizaje profundo}

Se diseñaron e implementaron tres arquitecturas de redes neuronales distintas para abordar la tarea de clasificación multiclase. Aunque las estrategias de extracción de características variaron, todas las redes compartieron un bloque clasificador final.


\subsubsection{Modelos convolucionales}

Para evaluar el rendimiento del aprendizaje de características espaciales sobre lo mapas multiómicos artificiales ($53 \times 53$), se implementaron dos modelos distintos, el primero una variante del modelo convolucional LeNet-5 y el segundo una arquitectura profunda con normalización por lotes. Ambas modelos buscan identificar patrones locales en la matriz reordenada, aunque con diferentes profundidades y funciones de activación.

\textit{A. Variante de LeNet-5}

Este modelo se adaptó de la clásica LeNet-5, que su propósito original para el que se diseñó es para el reconocimiento de dígitos \cite{lecun2002gradient}, para procesar la complejidad de los datos omicos. El modelo consta de dos bloques convolucionales secuenciales que funcionan como capas de extracción, el primer bloque con 6 filtros ($5 \times 5$) y el segundo con 16 filtros ($5 \times 5$) la configuracion se muestra en la Figura \ref{fig:LeNetMod}. La función de activación utilizada fue la tangente hiperbólica (tanh) y reducción de dimensionalidad Average Pooling ($2 \times 2$), preservando el flujo de información promedio de los vecindarios de características. Seguido de estos bloques se aplanó la salida y se sometió a un clasificador denso, dada la alta dimensionalidad de los datos de entrada, se utilizaron 4 capas ocultas consecutivas de 1000, 500, 120 y 84 neuronas, todas con activación tanh y por último una capa de clasificación con activación Softmax. Entre cada capa de convolución y densa se aplicó Dropout con una tasa de 0.1 para mitigar el sobreajuste, el optimizador usado fue Adam y un tamaño de lote (batch size) de 128.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Imagenes/Modelo_LeNetMod.png}
    \caption{Arquitectura de modelo LeNet modificado para clasificación de 32 tipos de cancer}
    \label{fig:LeNetMod}
\end{figure}

\textit{CNN profunda con normalización}

El segundo modelo se adaptó de acuerdo con el propuesto por Chuang et al. (2021) \cite{chuang2021convolutional}, la cual demostró una alta eficacia en la predicción de tipos de cáncer mediante la integración de datos ómicos y redes de interacción. Este modelo consta de tres bloques de extracción de características diseñados para capturar patrones no lineales complejos (Figura \ref{fig:CNN_prof}). Cada uno de los bloques está compuesto secuencialmente por una capa de convolución de 64 filtros, normalización por lotes (Batch Normalization) para estabilizar la distribución de las activaciones, una capa de Max-Pooling ($2 \times 2$) y Dropout. El primer bloque utiliza un campo receptivo mayor con kernel de $5 \times 5$, mientras que los dos bloques subsiguientes refinan las características con kernels de $3 \times 3$ con activación ReLU para mitigar el problema del desvanecimiento del gradiente. Posterior a las capas de convolución, se tiene una capa de aplanamiento seguido de capas profundas de 1000, 600 y 80 neuronas antes de la capa de clasificación final con función Softmax. El optimizador utilizado fue Adam con una tasa de aprendizaje de $1 \times 10^{-4}$ y un tamaño de lote reducido de 24.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Imagenes/Modelo_CNN_Prof.png}
    \caption{Arquitectura de modelo CNN modificado para clasificación de 32 tipos de cancer}
    \label{fig:CNN_prof}
\end{figure}


\subsubsection{modelos RNN}

Para evaluar la capacidad de los modelos recurrentes en la integración de datos multiómicos, se contrastaron dos estrategias de modelos secuenciales, un modelo de referencia basado en LSTM apiladas (para secuencias de características planas) y una arquitectura jerárquica multimodal (basada en GRU bidireccional).

\textit{Modelo LSTM Apilado}

Como primera estrategia, se implementó una modelo con arquitectura profunda basada en unidades de memoria de corto y largo plazo (LSTM) como se muestra en la Figura \ref{fig:RNN_sec}. Este modelo aborda el vector de características temporales completo como una única secuencia temporal continua. La extracción de características temporales se realiza mediante dos capas LSTM consecutivas: una capa inicial de 128 unidades configurada para retornar la secuencia completa, seguida de una segunda capa de 64 unidades que condensa la información en un vector de contexto global. La etapa de clasificación está compuesta por una red totalmente conectada de cuatro capas densas decrecientes de 1000, 500, 200 y 60 neuronas, la cual fue diseñada para modelar interacciones altamente no lineales a partir del estado oculto de la RNN, en ambas etapas se utilizó la función de activación tanh. Por último se tiene una capa de clasificación con función de activación Softmax.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Imagenes/Modelo_RNN_Sec.png}
    \caption{Arquitectura de modelo LSTM apilado para clasificación de 32 tipos de cancer}
    \label{fig:RNN_sec}
\end{figure}

\textit{Modelo RNN Multimodal Jerárquico}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Imagenes/Modelo_GRU_BI.png}
    \caption{Arquitectura de modelo RNN jerárquico para clasificación de 32 tipos de cáncer}
    \label{fig:Modelo_GRU_BI}
\end{figure}

Por otro lado, para mitigar los problemas de dispersión de señal en secuencias largas, se implementó una variante jerárquica optimizada denominada OmicsRNN (Figura \ref{fig:Modelo_GRU_BI}). A diferencia del modelo anterior, este modelo segrega inicialmente los datos en sus tres modalidades constitutivas (ARNm, miARN y metilación), en vectores latentes de 512 dimensiones mediante proyecciones densas independientes (Lineal $\rightarrow$ Batch Normalization $\rightarrow$ ReLU $\rightarrow$ Dropout). Esto permite al modelo aprender representaciones específicas para cada ómica antes de la integración. 

Posteriormente, estos embeddings latentes se organizaron en una secuencia corta de tres pasos ($T=3$) siguiendo el flujo de regulación biológica (Metilación $\rightarrow$ miARN $\rightarrow$ ARNm). La secuencia compacta alimenta una Unidad Recurrente Con Puerta (GRU) bidireccional de 2 capas con 256 unidades ocultas. La bidireccionalidad es crucial, ya que permite al modelo capturar tanto la regulación hacia adelante como las dependencias inversas o de retroalimentación biológica. Finalmente, el estado oculto concatenado de la GRU se procesa mediante un clasificador de una única capa densa de 128 neuronas con activación ReLU y regularización Droput.



\subsection{Modelos GNN}

Para explotar la topología relacional definida por la matriz de correlación de Pearson, se implementó un marco de modelado basado en grafos utilizando la biblioteca Pytorch Geometric. El grafo de entrada, definido por la matriz de características $X$ y la lista de aristas (edge index), fue procesado por tres arquitecturas distintas: una basada en atención jerárquica, una basada en transformadores y una convolucional profunda, esto para evaluar el impacto de diferentes mecanismos de agregación de vecindad (message passing) en la tarea de clasificación multiclase.

\textit{A. GAT (Graph Attention Network)}

La GAT se implementó con cuatro capas diseñadas con un esquema de atención piramidal, diseñada para gestionar la alta complejidad de las interacciones genéticas. La primera capa proyecta las características de entrada a un espacio latente de 1024 dimensiones distribuidas en 8 cabezales de atención, generando un intermedio masivo de 8,192 características por nodo. Para canalizar esta información, las capas subsiguientes reducen progresivamente la dimensionalidad (512 características $\times$ 4 cabezales) y la tercera a 512 dimensiones (256 características $\times$ 2 cabezales). Finalmente, la última capa consolida la información en un único cabezal que mapea directamente a las 32 dimensiones de salida correspondiente a las clases objetivo. Este flujo se estabiliza mediante normalización por lotes aplicada a las salidas de alta dimensionalidad antes de la activación LeakyReLU y Dropout del 0.1.

\textit{B. GTN (Graph Transformer Network)}

Para capturar dependencias globales complejas, se evaluó una arquitectura basada en Graph Transformer que mantiene una densidad de características constante a través de sus capas profundas. El modelo consta de tres bloques TransformerConv consecutivos, configuradas cada uno con 6 cabezales de atención y 80 canales por cabezal, resultando en una representación vectorial constante de 480 dimensiones por nodo a lo largo de la red. Esta preservación del ancho de banda permite al modelo refinar las representaciones sin pérdida de información prematura. La capa final transforma este vector de 480 dimensiones mediante un único cabezal de atención hacia los 32 logits de clasificación, Dado el alto número de parámetros implicados en los bloques de atención densa, se aplicó una tasa de Dropout del 0.5 junto con activaciones LeakyReLU para maximizar la capacidad de generalización.

\textit{C. GCN (Graph Convolutional Network)}

Se diseñó una GCN de 4 capas con una arquitectura de compresión piramidal (Figura \ref{fig:Modelo_GCN}). A diferencia de los modelos de ancho constante, esta variante fuerza al modelo a sintetizar  las características más relevantes reduciendo la dimensionalidad del espacio latente en cada salto del grafo. La primera  capa convolucional proyecta el vector de entrada a un espacio oculto de 600 dimensiones. Las capas subsiguientes aplican una reducción progresiva a 300 y 150 dimensiones respectivamente, obligando a la red a aprender representaciones cada vez más abstractas y compactas de los vecindarios biológicos. Finalmente, la cuarta capa proyecta el vector de 150 características hacia las 32 clases de salida. Cada etapa intermedia utiliza la función de activación ReLU y una regularización por Dropout del 0.3 para mitigar el sobreajuste durante la compresión de características.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Imagenes/Modelo_GCN.png}
    \caption{Flujo de procesamiento desde la entrada multi-ómica ($N=2,794$) hasta la clasificación de 32 tipos de cáncer. Los bloques representan la reducción jerárquica de dimensionalidad ($600 \rightarrow 300 \rightarrow 150 \rightarrow 32$), diseñada para la síntesis de representaciones latentes abstractas. Se indican las capas de convolución gráfica (bloques blancos) y la capa de salida (esferas amarillas), integrando regularización Dropout y activaciones ReLU en cada etapa de compresión.}
    \label{fig:Modelo_GCN}
\end{figure}


%\begin{itemize}
%    \item CNN: Describir la arquitectura (numero de capas, filtros, capas de agrupamiento, capas densas)
%    \item RNN: Especificar las capas y neuronas ocultas y la estructura de los datos en secuencia
%    \item GNN: Describir la arquitectura (GCN, GAT, GTN) numero de capas
%\end{itemize}

\subsection{Entrenamiento y validacion}


\begin{itemize}
    \item Division de los conjuntos de entrenamiento y validacion
    \item Hiperametros: Tasa de aprendizaje, optimizador, funcion de perdida, numero de epocas, sobre ajuste (dropout)
    \item Metricas de evaluacion utilizadas
\end{itemize}


\section{Resultados}
%%\label{}
- Rendimiento comparativo con tablas para mostrar las metricas de cada modelo y el tiempo de entrenamiento de cada uno
- Graficos como precision y perdida en entrenamiento y validacion y matrices de confusion



\section{Discusión}
%%\label{}

\section{Conclusiones}
%%\label{}


\section*{Acknowledgements}


%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix

\section{Appendix title 1}
%% \label{}

\section{Appendix title 2}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{elsarticle-num} 
\bibliography{Blibliografia}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%%\begin{thebibliography}{00}

%% \bibitem[Author(year)]{label}
%% For example:

%% \bibitem[Aladro et al.(2015)]{Aladro15} Aladro, R., Martín, S., Riquelme, D., et al. 2015, \aas, 579, A101


%%\end{thebibliography}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
