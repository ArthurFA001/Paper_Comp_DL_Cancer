\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{OMS_Cancer_2023_online}
\citation{NCI_CancerStats_2025_online}
\citation{siegel2023cancer}
\citation{dizon2024cancer}
\Newlabel{label1}{a}
\Newlabel{label2}{b}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introducción}{1}{section.1}\protected@file@percent }
\newlabel{Introducción}{{1}{1}{Introducción}{section.1}{}}
\citation{hanahan2011hallmarks}
\citation{zhuang2021multi}
\citation{acharya2024comprehensive}
\citation{massimino2023single}
\citation{hernandez2024methods}
\citation{kutlay2021integrative}
\citation{libbrecht2015machine}
\citation{lecun2015deep}
\citation{sartori2025comprehensive}
\citation{sartori2025comprehensive}
\citation{ye2021genomic,chuang2021convolutional}
\citation{parthasarathy2025novel,barbadilla2025predicting}
\citation{jiang2019semi,velickovic2017graph}
\citation{weinstein2013cancer}
\citation{colaprico2016tcgabiolinks}
\@writefile{toc}{\contentsline {section}{\numberline {2}Metodología}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Recopilación de datos}{4}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Numero datos multiómicos total (ARNm, miARN y metilación de ADN) y su distribución por cada tipo muestra tumoral y normal, incluidos en el proyecto Pan-cáncer de los 31 tipos de cancer}}{4}{figure.1}\protected@file@percent }
\newlabel{fig:muestras_TCGA}{{1}{4}{Numero datos multiómicos total (ARNm, miARN y metilación de ADN) y su distribución por cada tipo muestra tumoral y normal, incluidos en el proyecto Pan-cáncer de los 31 tipos de cancer}{figure.1}{}}
\citation{alharbi2025comparative}
\citation{robinson2010edger}
\citation{ritchie2015limma}
\citation{tibshirani1996regression}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Número datos multiómicos total (ARNm, miARN y metilación de ADN) y su distribución por cada tipo muestra tumoral y normal, incluidos en el proyecto Pan-cáncer }}{5}{figure.2}\protected@file@percent }
\newlabel{fig:muestrasxgenes}{{2}{5}{Número datos multiómicos total (ARNm, miARN y metilación de ADN) y su distribución por cada tipo muestra tumoral y normal, incluidos en el proyecto Pan-cáncer}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Preprocesamiento de datos}{5}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Número datos multiómicos total (ARNm, miARN y metilación de ADN) y su distribución por cada tipo muestra tumoral y normal, incluidos en el proyecto Pan-cáncer }}{5}{figure.3}\protected@file@percent }
\newlabel{fig:Proc_datos}{{3}{5}{Número datos multiómicos total (ARNm, miARN y metilación de ADN) y su distribución por cada tipo muestra tumoral y normal, incluidos en el proyecto Pan-cáncer}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Análisis de expresión génica diferencial (DGE)}{6}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Análisis de metilación diferencial}{6}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}LASSO}{7}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Integración y construcción de matriz multiómica}{7}{subsubsection.2.2.4}\protected@file@percent }
\citation{jeon2025investigating,fabbri2010epigenetics}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Estructuración de datos para modelos de aprendizaje profundo}{8}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Estructuración espacial 2D (CNN)}{8}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Estructuración secuencial (RNN)}{8}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Estructuración basada en grafos (GNN)}{9}{subsubsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Arquitecturas de los modelos de aprendizaje profundo}{9}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Modelos convolucionales}{9}{subsubsection.2.4.1}\protected@file@percent }
\citation{lecun2002gradient}
\citation{chuang2021convolutional}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Adaptación de la arquitectura LeNet-5. Esquema de la red convolucional jerárquica que ilustra la transición desde la matriz de entrada ($53 \times 53$) hacia la extracción de características mediante dos bloques de filtros ($6 \times 5 \times 5$ y $16 \times 5 \times 5$). El diagrama detalla la integración de Average Pooling ($2 \times 2$) y la fase de aplanamiento (Flatten) previa a un clasificador denso de cuatro capas ($1000 \rightarrow 500 \rightarrow 120 \rightarrow 84$) con activaciones tanh, culminando en la capa Softmax para la clasificación de los 32 subtipos tumorales.}}{10}{figure.4}\protected@file@percent }
\newlabel{fig:LeNetMod}{{4}{10}{Adaptación de la arquitectura LeNet-5. Esquema de la red convolucional jerárquica que ilustra la transición desde la matriz de entrada ($53 \times 53$) hacia la extracción de características mediante dos bloques de filtros ($6 \times 5 \times 5$ y $16 \times 5 \times 5$). El diagrama detalla la integración de Average Pooling ($2 \times 2$) y la fase de aplanamiento (Flatten) previa a un clasificador denso de cuatro capas ($1000 \rightarrow 500 \rightarrow 120 \rightarrow 84$) con activaciones tanh, culminando en la capa Softmax para la clasificación de los 32 subtipos tumorales}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Arquitectura de modelo CNN profunda. Esquema basado en la propuesta de Chuang et al. para la captura de patrones no lineales. El diagrama ilustra tres bloques de extracción de características con filtros de $64$ y reducción mediante Max-Pooling ($2 \times 2$), transitando desde kernels de $5 \times 5$ hacia $3 \times 3$. Se detalla el proceso de aplanamiento (Flatten) y la transición hacia un clasificador profundo de tres capas densas ($1000 \rightarrow 600 \rightarrow 80$) para la distinción de los 32 subtipos tumorales mediante una capa final Softmax.}}{11}{figure.5}\protected@file@percent }
\newlabel{fig:CNN_prof}{{5}{11}{Arquitectura de modelo CNN profunda. Esquema basado en la propuesta de Chuang et al. para la captura de patrones no lineales. El diagrama ilustra tres bloques de extracción de características con filtros de $64$ y reducción mediante Max-Pooling ($2 \times 2$), transitando desde kernels de $5 \times 5$ hacia $3 \times 3$. Se detalla el proceso de aplanamiento (Flatten) y la transición hacia un clasificador profundo de tres capas densas ($1000 \rightarrow 600 \rightarrow 80$) para la distinción de los 32 subtipos tumorales mediante una capa final Softmax}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}modelos RNN}{11}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Arquitectura de modelo LSTM apilado. Esquema del modelo de procesamiento secuencial para el vector de $2,794$ características. El diagrama ilustra la etapa de extracción recurrente mediante dos capas LSTM ($128$ y $64$ unidades) para la generación de un vector de contexto global. Se detalla el clasificador profundo compuesto por cuatro capas densas ($1000 \rightarrow 500 \rightarrow 200 \rightarrow 60$) con activación tanh, culminando en la capa de salida Softmax para la clasificación de los 32 tipos tumorales.}}{12}{figure.6}\protected@file@percent }
\newlabel{fig:RNN_sec}{{6}{12}{Arquitectura de modelo LSTM apilado. Esquema del modelo de procesamiento secuencial para el vector de $2,794$ características. El diagrama ilustra la etapa de extracción recurrente mediante dos capas LSTM ($128$ y $64$ unidades) para la generación de un vector de contexto global. Se detalla el clasificador profundo compuesto por cuatro capas densas ($1000 \rightarrow 500 \rightarrow 200 \rightarrow 60$) con activación tanh, culminando en la capa de salida Softmax para la clasificación de los 32 tipos tumorales}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Arquitectura de modelo RNN jerárquico. Esquema del modelo basado en el flujo de regulación biológica. Se ilustra la proyección lineal independiente de las tres modalidades ($393$ Metilación, $1,881$ miARN, $393$ ARNm) hacia subespacios latentes de $512$ dimensiones. El diagrama detalla la construcción de la secuencia apilada ($T=3$) y su procesamiento mediante una red recurrente bidireccional de dos capas ($256$ unidades), finalizando en un clasificador profundo de $128$ neuronas para la distinción de los 32 subtipos tumorales.}}{13}{figure.7}\protected@file@percent }
\newlabel{fig:Modelo_GRU_BI}{{7}{13}{Arquitectura de modelo RNN jerárquico. Esquema del modelo basado en el flujo de regulación biológica. Se ilustra la proyección lineal independiente de las tres modalidades ($393$ Metilación, $1,881$ miARN, $393$ ARNm) hacia subespacios latentes de $512$ dimensiones. El diagrama detalla la construcción de la secuencia apilada ($T=3$) y su procesamiento mediante una red recurrente bidireccional de dos capas ($256$ unidades), finalizando en un clasificador profundo de $128$ neuronas para la distinción de los 32 subtipos tumorales}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Modelos GNN}{13}{subsection.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Arquitectura GCN (Graph Convolutional Network). Flujo de procesamiento desde la entrada multi-ómica ($N=2,794$) hasta la clasificación de 32 tipos de cáncer. Los bloques representan la reducción jerárquica de dimensionalidad ($600 \rightarrow 300 \rightarrow 150 \rightarrow 32$), diseñada para la síntesis de representaciones latentes abstractas. Se indican las capas de convolución gráfica (bloques blancos) y la capa de salida (esferas amarillas), integrando regularización Dropout y activaciones ReLU en cada etapa de compresión.}}{14}{figure.8}\protected@file@percent }
\newlabel{fig:Modelo_GCN}{{8}{14}{Arquitectura GCN (Graph Convolutional Network). Flujo de procesamiento desde la entrada multi-ómica ($N=2,794$) hasta la clasificación de 32 tipos de cáncer. Los bloques representan la reducción jerárquica de dimensionalidad ($600 \rightarrow 300 \rightarrow 150 \rightarrow 32$), diseñada para la síntesis de representaciones latentes abstractas. Se indican las capas de convolución gráfica (bloques blancos) y la capa de salida (esferas amarillas), integrando regularización Dropout y activaciones ReLU en cada etapa de compresión}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Arquitectura GAT (Graph Attention Network). Representación del flujo de atención multi-cabezal para la clasificación de 32 tipos de cáncer. El diagrama detalla la arquitectura de compresión jerárquica mediante capas GATConv, partiendo de una proyección inicial de alta densidad ($1024 \times 8$ cabezales) hacia capas de dimensiones reducidas ($512 \times 4$ y $256 \times 2$). Las esferas amarillas representan la consolidación final en un único cabezal de atención para el mapeo de las clases objetivo. Se indica la integración de Batch Normalization y activación LeakyReLU para la estabilidad del entrenamiento.}}{15}{figure.9}\protected@file@percent }
\newlabel{fig:Modelo_GAT}{{9}{15}{Arquitectura GAT (Graph Attention Network). Representación del flujo de atención multi-cabezal para la clasificación de 32 tipos de cáncer. El diagrama detalla la arquitectura de compresión jerárquica mediante capas GATConv, partiendo de una proyección inicial de alta densidad ($1024 \times 8$ cabezales) hacia capas de dimensiones reducidas ($512 \times 4$ y $256 \times 2$). Las esferas amarillas representan la consolidación final en un único cabezal de atención para el mapeo de las clases objetivo. Se indica la integración de Batch Normalization y activación LeakyReLU para la estabilidad del entrenamiento}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Configuración experimental y métricas de evaluación}{15}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Entorno de hardware y software}{15}{subsubsection.2.6.1}\protected@file@percent }
\citation{chicco2020advantages}
\citation{johnson2019survey}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Arquitectura Graph Transformer (GTN). Diagrama del flujo de atención mediante bloques \textit  {TransformerConv} ($80 \times 6$ cabezales). A diferencia de los modelos piramidales, esta red mantiene una densidad constante de 480 dimensiones para el refinamiento de características globales. Se detalla la transición desde la entrada ($N=2,794$) hasta la salida de 32 clases, integrando Dropout de 0.5 y activaciones LeakyReLU.}}{16}{figure.10}\protected@file@percent }
\newlabel{fig:Modelo_GTN}{{10}{16}{Arquitectura Graph Transformer (GTN). Diagrama del flujo de atención mediante bloques \textit {TransformerConv} ($80 \times 6$ cabezales). A diferencia de los modelos piramidales, esta red mantiene una densidad constante de 480 dimensiones para el refinamiento de características globales. Se detalla la transición desde la entrada ($N=2,794$) hasta la salida de 32 clases, integrando Dropout de 0.5 y activaciones LeakyReLU}{figure.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Partición de Datos y Protocolo}{16}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Métricas de evaluación}{16}{subsubsection.2.6.3}\protected@file@percent }
\bibstyle{elsarticle-num}
\bibdata{Blibliografia}
\bibcite{OMS_Cancer_2023_online}{{1}{}{{}}{{}}}
\bibcite{NCI_CancerStats_2025_online}{{2}{}{{}}{{}}}
\bibcite{siegel2023cancer}{{3}{}{{}}{{}}}
\bibcite{dizon2024cancer}{{4}{}{{}}{{}}}
\bibcite{hanahan2011hallmarks}{{5}{}{{}}{{}}}
\bibcite{zhuang2021multi}{{6}{}{{}}{{}}}
\bibcite{acharya2024comprehensive}{{7}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Resultados}{17}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Discusión}{17}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusiones}{17}{section.5}\protected@file@percent }
\@writefile{toc}{\let\numberline\tmptocnumberline}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix ~A}Appendix title 1}{17}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {Appendix ~B}Appendix title 2}{17}{appendix.B}\protected@file@percent }
\bibcite{massimino2023single}{{8}{}{{}}{{}}}
\bibcite{hernandez2024methods}{{9}{}{{}}{{}}}
\bibcite{kutlay2021integrative}{{10}{}{{}}{{}}}
\bibcite{libbrecht2015machine}{{11}{}{{}}{{}}}
\bibcite{lecun2015deep}{{12}{}{{}}{{}}}
\bibcite{sartori2025comprehensive}{{13}{}{{}}{{}}}
\bibcite{ye2021genomic}{{14}{}{{}}{{}}}
\bibcite{chuang2021convolutional}{{15}{}{{}}{{}}}
\bibcite{parthasarathy2025novel}{{16}{}{{}}{{}}}
\bibcite{barbadilla2025predicting}{{17}{}{{}}{{}}}
\bibcite{jiang2019semi}{{18}{}{{}}{{}}}
\bibcite{velickovic2017graph}{{19}{}{{}}{{}}}
\bibcite{weinstein2013cancer}{{20}{}{{}}{{}}}
\bibcite{colaprico2016tcgabiolinks}{{21}{}{{}}{{}}}
\bibcite{alharbi2025comparative}{{22}{}{{}}{{}}}
\bibcite{robinson2010edger}{{23}{}{{}}{{}}}
\bibcite{ritchie2015limma}{{24}{}{{}}{{}}}
\bibcite{tibshirani1996regression}{{25}{}{{}}{{}}}
\bibcite{jeon2025investigating}{{26}{}{{}}{{}}}
\bibcite{fabbri2010epigenetics}{{27}{}{{}}{{}}}
\bibcite{lecun2002gradient}{{28}{}{{}}{{}}}
\bibcite{johnson2019survey}{{29}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{19}
